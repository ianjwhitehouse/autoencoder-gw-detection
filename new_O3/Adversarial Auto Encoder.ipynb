{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f98d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240122163835\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "import string\n",
    "from random import choices, choice\n",
    "from datetime import datetime\n",
    "\n",
    "INPUT_SIZE = [512, 1024, 2048, 4096]\n",
    "INPUT_SIZE = [4096]\n",
    "HIDDEN_LAYERS = [1, 2]\n",
    "HIDDEN_LAYER_DIVISOR = [2, 4, 8]\n",
    "Z_LAYER_DIVISOR = [4, 8, 16, 32]\n",
    "DROPOUT = [.1, .25, .5]\n",
    "LEARNING_RATE = [0.000001, 0.00001, 0.0001]\n",
    "BATCH_SIZE = [8, 16, 32]\n",
    "FOLLOWER = False\n",
    "\n",
    "DATA_SET = \"O3 Run\"\n",
    "TYPE = \"Adversarial AE\"\n",
    "TIME_LENGTH = 1\n",
    "DIFFERENCE_THRESHS = [.8 + 0.2 * i for i in range(25)]\n",
    "ATTEMPT = 0 # Set later, doesn't make a difference\n",
    "\n",
    "# New masking section\n",
    "MASKING = [False, True, True, True]\n",
    "MASK_SIZE = [.25, .5, .75]\n",
    "\n",
    "name = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "model_final_stats = {\"name\": name, \"dataset\": DATA_SET, \"type\": TYPE, \"len\": TIME_LENGTH}\n",
    "\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5e13afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_113263/2516016999.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Assign whether follower or not based on the previous models attempt\n",
    "import pandas as pd\n",
    "\n",
    "# Previous failed\n",
    "previous_failed = False\n",
    "\n",
    "try:\n",
    "    open(\"failed\", \"x\")\n",
    "except FileExistsError:\n",
    "    if FOLLOWER:\n",
    "        crash()\n",
    "    else:\n",
    "        previous_failed = True\n",
    "\n",
    "try:\n",
    "    previous_trials = pd.read_csv(\"ae_trials.csv\").to_dict(\"records\")\n",
    "    if len(previous_trials) % 2 == 1:\n",
    "        print(1)\n",
    "        FOLLOWER = True\n",
    "        ATTEMPT = previous_trials[-1][\"attempt\"]\n",
    "    elif previous_trials[-1][\"attempt\"] == 4:\n",
    "        print(2)\n",
    "        FOLLOWER = False\n",
    "        ATTEMPT = 0\n",
    "    else:\n",
    "        print(3)\n",
    "        FOLLOWER = True\n",
    "        ATTEMPT = previous_trials[-1][\"attempt\"] + 1\n",
    "    \n",
    "    if previous_failed:\n",
    "        print(\"Prev Failed\")\n",
    "        FOLLOWER = False\n",
    "        ATTEMPT = 0\n",
    "        \n",
    "        \n",
    "except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "    print(4)\n",
    "    FOLLOWER = False\n",
    "    ATTEMPT = 0\n",
    "\n",
    "model_final_stats[\"attempt\"] = ATTEMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0845994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '20240122163835', 'dataset': 'O3 Run', 'type': 'Adversarial AE', 'len': 1, 'attempt': 0, 'leader': None, 'input_size': 4096, 'hidden_layers': 2, 'hidden_layers_divisor': 2, 'z_layer_divisor': 16, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 32, 'masking': True, 'mask_size': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Pick hyperparameters\n",
    "hyper_params = {\n",
    "    \"input_size\": choice(INPUT_SIZE),\n",
    "    \"hidden_layers\": choice(HIDDEN_LAYERS),\n",
    "    \"hidden_layers_divisor\": choice(HIDDEN_LAYER_DIVISOR),\n",
    "    \"z_layer_divisor\": choice(Z_LAYER_DIVISOR),\n",
    "    \"dropout\": choice(DROPOUT),\n",
    "    \"learning_rate\": choice(LEARNING_RATE),\n",
    "    \"batch_size\": choice(BATCH_SIZE),\n",
    "    \"masking\": choice(MASKING),\n",
    "    \"mask_size\": choice(MASK_SIZE)\n",
    "}\n",
    "hyper_params[\"input_size\"] *= TIME_LENGTH\n",
    "hyper_params[\"hidden_layers_divisor\"] *= TIME_LENGTH\n",
    "hyper_params[\"z_layer_divisor\"] *= TIME_LENGTH\n",
    "model_final_stats[\"leader\"] = None\n",
    "\n",
    "if FOLLOWER:\n",
    "    previous_models = pd.read_csv(\"ae_trials.csv\", keep_default_na=False).to_dict(\"records\")\n",
    "    for model in previous_models[::-1]: # We want to find the original leader not a fake leader\n",
    "        if not model[\"leader\"]:\n",
    "            model_final_stats[\"leader\"] = model[\"name\"]\n",
    "            break\n",
    "    \n",
    "    for key in hyper_params.keys():\n",
    "        hyper_params[key] = model[key]\n",
    "\n",
    "model_final_stats.update(hyper_params)\n",
    "print(model_final_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f0c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "from pickle import load\n",
    "from random import sample\n",
    "\n",
    "x_tests, y_tests = load(open(\"data/test_%d.pickle\" % hyper_params[\"input_size\"], \"rb\"))\n",
    "x_tests = [[x - 0.5 for x in sample] for sample in x_tests[ATTEMPT]]\n",
    "y_tests = y_tests[ATTEMPT]\n",
    "\n",
    "x_train = load(open(\"data/train_%d.pickle\" % hyper_params[\"input_size\"], \"rb\"))\n",
    "x_train = [[x - 0.5 for x in sample] for sample in x_train]\n",
    "\n",
    "# Mask x_tests and y_tests\n",
    "if hyper_params[\"masking\"]:\n",
    "    # print(int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"]), hyper_params[\"input_size\"])\n",
    "    # print(len(x_tests))\n",
    "    x_tests = [[x, [x[i] for i in sorted(sample(range(len(x)), int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))]] for x in x_tests]\n",
    "    x_train = [[x, [x[i] for i in sorted(sample(range(len(x)), int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))]] for x in x_train]\n",
    "\n",
    "x_tests, y_tests = [x_tests], [y_tests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336ab493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 16:38:49.114667: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-22 16:38:49.308471: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-22 16:38:52.226274: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 4096)              8392704   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1365)              5592405   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1365)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               349696    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14334805 (54.68 MB)\n",
      "Trainable params: 14334805 (54.68 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 16:38:53.948205: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2024-01-22 16:38:53.948302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: au303074\n",
      "2024-01-22 16:38:53.948308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: au303074\n",
      "2024-01-22 16:38:53.948383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 535.146.2\n",
      "2024-01-22 16:38:53.948398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 535.129.3\n",
      "2024-01-22 16:38:53.948402: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:312] kernel version 535.129.3 does not match DSO version 535.146.2 -- cannot find working devices in this configuration\n"
     ]
    }
   ],
   "source": [
    "# Create encoder model\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "encoder = models.Sequential()\n",
    "\n",
    "if hyper_params[\"masking\"]:\n",
    "    encoder.add(layers.Input(shape=int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))\n",
    "else:\n",
    "    encoder.add(layers.Input(shape=hyper_params[\"input_size\"]))\n",
    "\n",
    "for i in range(hyper_params[\"hidden_layers\"]):\n",
    "    encoder.add(layers.Dense(hyper_params[\"input_size\"]//(hyper_params[\"hidden_layers_divisor\"] * i + 1), activation=\"elu\"))\n",
    "    encoder.add(layers.Dropout(hyper_params[\"dropout\"]))\n",
    "encoder.add(layers.Dense(hyper_params[\"input_size\"]//hyper_params[\"z_layer_divisor\"]))\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9434c80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 1365)              350805    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1365)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4096)              5595136   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22727253 (86.70 MB)\n",
      "Trainable params: 22727253 (86.70 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create decoder model\n",
    "decoder = models.Sequential()\n",
    "decoder.add(layers.Input(hyper_params[\"input_size\"]//hyper_params[\"z_layer_divisor\"]))\n",
    "for i in list(range(hyper_params[\"hidden_layers\"]))[::-1]:\n",
    "    decoder.add(layers.Dense(hyper_params[\"input_size\"]//(hyper_params[\"hidden_layers_divisor\"] * i + 1), activation=\"elu\"))\n",
    "    decoder.add(layers.Dropout(hyper_params[\"dropout\"]))\n",
    "decoder.add(layers.Dense(hyper_params[\"input_size\"]))\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e5cbafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 256)               14334805  \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 4096)              22727253  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37062058 (141.38 MB)\n",
      "Trainable params: 37062058 (141.38 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create autoencoder model\n",
    "autoencoder = models.Sequential()\n",
    "\n",
    "if hyper_params[\"masking\"]:\n",
    "    autoencoder.add(layers.Input(shape=int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))\n",
    "else:\n",
    "    autoencoder.add(layers.Input(shape=hyper_params[\"input_size\"]))\n",
    "\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "813dd5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 1365)              350805    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1365)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4096)              5595136   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 4097      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5950038 (22.70 MB)\n",
      "Trainable params: 5950038 (22.70 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create discriminator\n",
    "discriminator = models.Sequential()\n",
    "discriminator.add(layers.Input(shape=hyper_params[\"input_size\"]//hyper_params[\"z_layer_divisor\"]))\n",
    "for i in list(range(hyper_params[\"hidden_layers\"]))[::-1]:\n",
    "    discriminator.add(layers.Dense(hyper_params[\"input_size\"]//(hyper_params[\"hidden_layers_divisor\"] * i + 1), activation=\"elu\"))\n",
    "    discriminator.add(layers.Dropout(hyper_params[\"dropout\"]))\n",
    "discriminator.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82e73e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\n",
    "from tensorflow.keras.models import clone_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "ae_opt = optimizers.Adam(learning_rate=hyper_params[\"learning_rate\"])\n",
    "gen_opt = optimizers.Adam(learning_rate=hyper_params[\"learning_rate\"])\n",
    "disc_opt = optimizers.Adam(learning_rate=hyper_params[\"learning_rate\"])\n",
    "\n",
    "bin_cross_entropy = BinaryCrossentropy(from_logits=False)\n",
    "mse = MeanSquaredError()\n",
    "\n",
    "def generate_real_z(percent_normal, variance, batch_length):\n",
    "    z_normal = np.random.randn(batch_length, hyper_params[\"input_size\"]//hyper_params[\"z_layer_divisor\"])\n",
    "    z_random = np.random.rand(batch_length, hyper_params[\"input_size\"]//hyper_params[\"z_layer_divisor\"]) * variance * 3\n",
    "    z = percent_normal * z_normal * variance\n",
    "    return z + (1 - percent_normal) * (z_random - ( 0.5 * variance * 3))\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    return bin_cross_entropy(np.zeros_like(fake), fake) + bin_cross_entropy(np.ones_like(real), real)\n",
    "\n",
    "def generator_loss(fake):\n",
    "    return bin_cross_entropy(np.ones_like(fake), fake)\n",
    "\n",
    "def train_batch(batch_x, batch_y=None):\n",
    "    global autoencoder # for whatever reason, train_epochs needs these and I didn't see a point in only adding it to one\n",
    "    global discriminator\n",
    "    global generator\n",
    "    \n",
    "    with tf.GradientTape() as ae_tape:\n",
    "        ae_predict = autoencoder(batch_x, training=True)\n",
    "\n",
    "        if batch_y is not None:\n",
    "            ae_loss = mse(batch_y, ae_predict)\n",
    "        else:\n",
    "            ae_loss = mse(batch_x, ae_predict)\n",
    "    \n",
    "    gradient_ae = ae_tape.gradient(ae_loss, autoencoder.trainable_variables)\n",
    "    ae_opt.apply_gradients(zip(gradient_ae, autoencoder.trainable_variables))\n",
    "    \n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        fake_dist = encoder(batch_x, training=True)\n",
    "        real_dist = generate_real_z(1, 5, len(batch_x))\n",
    "        \n",
    "        real_out = discriminator(real_dist, training=True)\n",
    "        fake_out = discriminator(fake_dist, training=True)\n",
    "\n",
    "        disc_loss = discriminator_loss(real_out, fake_out)\n",
    "\n",
    "    gradient_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    disc_opt.apply_gradients(zip(gradient_disc, discriminator.trainable_variables))\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        fake_dist = encoder(batch_x, training=True)\n",
    "        real_dist = generate_real_z(1, 5, len(batch_x))\n",
    "        \n",
    "        real_out = discriminator(real_dist, training=True)\n",
    "        fake_out = discriminator(fake_dist, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_out)\n",
    "\n",
    "    gradient_gen = gen_tape.gradient(gen_loss, encoder.trainable_variables)\n",
    "    gen_opt.apply_gradients(zip(gradient_gen, encoder.trainable_variables))\n",
    "\n",
    "def train_epochs(x_train, epochs, batch_size, validation_split=0, memory_for_earlystopping=-1):\n",
    "    # Saves every x epochs, where x is save and is greater than 0\n",
    "    \n",
    "    global autoencoder # for whatever reason, it needs these\n",
    "    global discriminator\n",
    "    global generator\n",
    "    \n",
    "    return_dict = {}\n",
    "    \n",
    "    if validation_split > 0:\n",
    "        validation_split = int(len(x_train) * validation_split)\n",
    "        x_val = x_train[:validation_split]\n",
    "        \n",
    "        if hyper_params[\"masking\"]:\n",
    "            y_val = [x[0] for x in x_val]\n",
    "            y_val = np.array(y_val).reshape(validation_split, hyper_params[\"input_size\"])\n",
    "            \n",
    "            x_val = [x[1] for x in x_val]\n",
    "            x_val = np.array(x_val).reshape(validation_split, int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"]))\n",
    "        else:\n",
    "            x_val = np.array(x_val).reshape(validation_split, hyper_params[\"input_size\"])\n",
    "        \n",
    "        x_train = x_train[validation_split:]\n",
    "        return_dict[\"ae_loss\"] = []\n",
    "        return_dict[\"disc_loss\"] = []\n",
    "        return_dict[\"gen_loss\"] = []\n",
    "    \n",
    "    number_of_batches = len(x_train)//batch_size\n",
    "    \n",
    "    if memory_for_earlystopping != -1:\n",
    "            return_dict[\"saves\"] = []\n",
    "    \n",
    "    shuffle(x_train)\n",
    "    batches = [x_train[b::number_of_batches] for b in range(number_of_batches)]\n",
    "\n",
    "    if hyper_params[\"masking\"]:\n",
    "        batches_x = [[x[1] for x in b] for b in batches]\n",
    "        batches_x = [np.array(b).reshape(len(b), int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])) for b in batches_x]\n",
    "        \n",
    "        batches_y = [[x[0] for x in b] for b in batches]\n",
    "        batches_y = [np.array(b).reshape(len(b), hyper_params[\"input_size\"]) for b in batches_y]\n",
    "    else:\n",
    "        batches = [np.array(b).reshape(len(b), hyper_params[\"input_size\"]) for b in batches]\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        print(\"Epoch %s Running...\" % (e), end=\"\")\n",
    "\n",
    "        if hyper_params[\"masking\"]:\n",
    "            for bx, by in zip(batches_x, batches_y):\n",
    "                train_batch(bx, batch_y=by)\n",
    "        else:\n",
    "            for b in batches:\n",
    "                train_batch(b)\n",
    "        \n",
    "        if validation_split > 0:\n",
    "            ae_predict = autoencoder(x_val)\n",
    "            if hyper_params[\"masking\"]:\n",
    "                ae_loss = mse(y_val, ae_predict)\n",
    "            else:\n",
    "                ae_loss = mse(x_val, ae_predict)\n",
    "            return_dict[\"ae_loss\"].append(ae_loss.numpy())\n",
    "            print(\"Done; AE Loss: %s\" % return_dict[\"ae_loss\"][-1], end=\"; \")\n",
    "            \n",
    "            fake_dist = encoder(x_val)\n",
    "            real_dist = 10 * np.random.randn(len(x_val), hyper_params[\"input_size\"]//hyper_params[\"z_layer_divisor\"])\n",
    "        \n",
    "            real_out = discriminator(real_dist)\n",
    "            fake_out = discriminator(fake_dist)\n",
    "\n",
    "            disc_loss = discriminator_loss(real_out, fake_out)\n",
    "            gen_loss = generator_loss(fake_out)\n",
    "            return_dict[\"disc_loss\"].append(disc_loss.numpy())\n",
    "            return_dict[\"gen_loss\"].append(gen_loss.numpy())\n",
    "            print(\"Disc Loss: %s; Gen Loss: %s\" % (return_dict[\"disc_loss\"][-1], return_dict[\"gen_loss\"][-1]))\n",
    "        else:\n",
    "            print(\"Done;\")\n",
    "        \n",
    "        if memory_for_earlystopping != -1:\n",
    "            if len(return_dict[\"saves\"]) == memory_for_earlystopping and return_dict[\"ae_loss\"][-1] > return_dict[\"saves\"][0][0] and min([v[0] for v in return_dict[\"saves\"]]) == return_dict[\"saves\"][0][0]:\n",
    "                best_model = return_dict[\"saves\"][0]\n",
    "                autoencoder = best_model[1]\n",
    "                discriminator = best_model[2]\n",
    "                del return_dict[\"ae_loss\"][-1 * memory_for_earlystopping:]\n",
    "                del return_dict[\"disc_loss\"][-1 * memory_for_earlystopping:]\n",
    "                del return_dict[\"gen_loss\"][-1 * memory_for_earlystopping:]\n",
    "                break;\n",
    "            else:\n",
    "                return_dict[\"saves\"].append((return_dict[\"ae_loss\"][-1], clone_model(autoencoder), clone_model(discriminator)))\n",
    "                if len(return_dict[\"saves\"]) > memory_for_earlystopping:\n",
    "                    del return_dict[\"saves\"][0]\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f44aa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Running..."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shuffle\n\u001b[0;32m----> 5\u001b[0m fit_run \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epochs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyper_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_for_earlystopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mae_saved/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n\u001b[1;32m     11\u001b[0m discriminator\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mae_saved/disc_\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n",
      "Cell \u001b[0;32mIn[9], line 115\u001b[0m, in \u001b[0;36mtrain_epochs\u001b[0;34m(x_train, epochs, batch_size, validation_split, memory_for_earlystopping)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hyper_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasking\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m bx, by \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batches_x, batches_y):\n\u001b[0;32m--> 115\u001b[0m         \u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batches:\n",
      "Cell \u001b[0;32mIn[9], line 51\u001b[0m, in \u001b[0;36mtrain_batch\u001b[0;34m(batch_x, batch_y)\u001b[0m\n\u001b[1;32m     47\u001b[0m     fake_out \u001b[38;5;241m=\u001b[39m discriminator(fake_dist, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     49\u001b[0m     disc_loss \u001b[38;5;241m=\u001b[39m discriminator_loss(real_out, fake_out)\n\u001b[0;32m---> 51\u001b[0m gradient_disc \u001b[38;5;241m=\u001b[39m \u001b[43mdisc_tape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisc_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m disc_opt\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradient_disc, discriminator\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m gen_tape:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1063\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1057\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1058\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1059\u001b[0m           output_gradients))\n\u001b[1;32m   1060\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1061\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1063\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1072\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:146\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    144\u001b[0m     gradient_name_scope \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m forward_pass_name_scope \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, \u001b[38;5;241m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/math_grad.py:1656\u001b[0m, in \u001b[0;36m_SelectGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1654\u001b[0m c \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1655\u001b[0m x \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1656\u001b[0m zeros \u001b[38;5;241m=\u001b[39m \u001b[43marray_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, array_ops\u001b[38;5;241m.\u001b[39mwhere(c, grad, zeros), array_ops\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[1;32m   1658\u001b[0m     c, zeros, grad))\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:2901\u001b[0m, in \u001b[0;36mzeros_like\u001b[0;34m(tensor, dtype, name, optimize)\u001b[0m\n\u001b[1;32m   2864\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros_like\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   2865\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39mregister_unary_elementwise_api\n\u001b[1;32m   2866\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m   2867\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mzeros_like\u001b[39m(tensor, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   2868\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a tensor with all elements set to zero.\u001b[39;00m\n\u001b[1;32m   2869\u001b[0m \n\u001b[1;32m   2870\u001b[0m \u001b[38;5;124;03m  See also `tf.zeros`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2899\u001b[0m \u001b[38;5;124;03m    A `Tensor` with all elements set to zero.\u001b[39;00m\n\u001b[1;32m   2900\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2901\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mzeros_like_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:2798\u001b[0m, in \u001b[0;36m_tag_zeros_tensor.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2798\u001b[0m   tensor \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2799\u001b[0m   tensor\u001b[38;5;241m.\u001b[39m_is_zeros_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2800\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:2953\u001b[0m, in \u001b[0;36mzeros_like_impl\u001b[0;34m(tensor, dtype, name, optimize)\u001b[0m\n\u001b[1;32m   2950\u001b[0m \u001b[38;5;129m@_tag_zeros_tensor\u001b[39m\n\u001b[1;32m   2951\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mzeros_like_impl\u001b[39m(tensor, dtype, name, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   2952\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Internal implementation for the v1/v2 zeros_like API calls.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2953\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname_scope\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzeros_like\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m name:\n\u001b[1;32m   2954\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor_util\u001b[38;5;241m.\u001b[39mis_tf_type(tensor):\n\u001b[1;32m   2955\u001b[0m       tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(tensor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:6207\u001b[0m, in \u001b[0;36mname_scope\u001b[0;34m(name, default_name, values, skip_on_eager)\u001b[0m\n\u001b[1;32m   6204\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m internal_name_scope_v1(name, default_name, values)\n\u001b[1;32m   6206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_on_eager:\n\u001b[0;32m-> 6207\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNullContextmanager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6209\u001b[0m name \u001b[38;5;241m=\u001b[39m default_name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m name\n\u001b[1;32m   6210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values:\n\u001b[1;32m   6211\u001b[0m   \u001b[38;5;66;03m# The presence of a graph tensor in `values` overrides the context.\u001b[39;00m\n\u001b[1;32m   6212\u001b[0m   \u001b[38;5;66;03m# TODO(slebedev): this is Keras-specific and should be removed.\u001b[39;00m\n\u001b[1;32m   6213\u001b[0m   \u001b[38;5;66;03m# pylint: disable=unidiomatic-typecheck\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:171\u001b[0m, in \u001b[0;36mNullContextmanager.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNullContextmanager\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    174\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit model and save weights\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "\n",
    "fit_run = train_epochs(\n",
    "    x_train, 300, hyper_params[\"batch_size\"],\n",
    "    validation_split=0.1, memory_for_earlystopping=5\n",
    ")\n",
    "\n",
    "autoencoder.save_weights(\"ae_saved/%s\" % name)\n",
    "discriminator.save_weights(\"ae_saved/disc_%s\" % name)\n",
    "final_loss = fit_run[\"ae_loss\"][-1]\n",
    "\n",
    "ae_res = {\"gen_loss\": fit_run[\"gen_loss\"][-1], \"disc_loss\": fit_run[\"disc_loss\"][-1], \"val_loss\": final_loss, \"epochs\": len(fit_run[\"ae_loss\"])}\n",
    "model_final_stats.update(ae_res)\n",
    "\n",
    "print(ae_res)\n",
    "plt.plot(range(len(fit_run[\"ae_loss\"])), fit_run[\"ae_loss\"])\n",
    "plt.title(\"Autoencoder Loss: %s, %s\" % (TYPE, name))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Loss (MSE)\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(fit_run[\"gen_loss\"])), fit_run[\"gen_loss\"])\n",
    "plt.title(\"Generator Loss: %s, %s\" % (TYPE, name))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Loss (Binary cross-entropy)\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(fit_run[\"disc_loss\"])), fit_run[\"disc_loss\"])\n",
    "plt.title(\"Discriminator Loss: %s, %s\" % (TYPE, name))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Loss (Binary cross-entropy)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf643d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save splits of z layers, run difference classifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from statistics import mean\n",
    "\n",
    "# Copied from latent layer classifiers\n",
    "def save_results(name, hyper_params, metrics, model_name, auc=False):\n",
    "    try:\n",
    "        previous_trials = pd.read_csv(\"latent_trials.csv\").to_dict(\"records\")\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        previous_trials = []\n",
    "\n",
    "    model_final_stats = {\"Classifier\": name, \"Based on AE\": model_name}\n",
    "    model_final_stats.update(hyper_params)\n",
    "    \n",
    "    model_final_stats[\"precision\"] = metrics[0]\n",
    "    model_final_stats[\"recall\"] = metrics[1]\n",
    "    model_final_stats[\"f-score\"] = metrics[2]\n",
    "    \n",
    "    if auc:\n",
    "        model_final_stats[\"auc\"] = metrics[3]\n",
    "    \n",
    "    previous_trials.append(model_final_stats)\n",
    "    pd.DataFrame(previous_trials).to_csv(\"latent_trials.csv\", index=None)\n",
    "    \n",
    "    print(model_final_stats)\n",
    "\n",
    "for split, x_test, y_test in zip(range(len(x_tests)), x_tests, y_tests):\n",
    "\n",
    "    if hyper_params[\"masking\"]:\n",
    "        z_layers = encoder.predict(np.array([x[1] for x in x_test]).reshape(len(x_test), int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))\n",
    "    else:\n",
    "        z_layers = encoder.predict(np.array(x_test).reshape(len(x_test), hyper_params[\"input_size\"]))\n",
    "    \n",
    "    outputs = decoder.predict(z_layers).tolist()\n",
    "    print(y_test[0])\n",
    "    class_true = [[np.argmax(y)] for y in y_test]\n",
    "    \n",
    "    z_layers = [y + z.tolist() for z, y in zip(z_layers, class_true)]\n",
    "    pd.DataFrame(z_layers).to_pickle(\"z_layers/%s.pickle.gzip\" % (name))\n",
    "    print(\"Saved z-layers for %s\" % split)\n",
    "    \n",
    "    for i, sigma in enumerate(DIFFERENCE_THRESHS):\n",
    "        results = [[], []]\n",
    "        for y_pred, y_true, y_class in zip(outputs, x_test, class_true):\n",
    "            results[0].append(y_class)\n",
    "            results[1].append(1 if mse(y_pred, y_true[0]).numpy() > sigma * final_loss else 0)\n",
    "        save_results(\"ReconstructionThreshold\", {\"sigma\": sigma}, list(precision_recall_fscore_support(results[0], results[1], average=\"binary\"))[:3], name)\n",
    "    print(\"Done with split %s\" % split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eedf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot five graphs\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc(\"font\", size=MEDIUM_SIZE)\n",
    "plt.rc(\"axes\", titlesize=MEDIUM_SIZE, labelsize=MEDIUM_SIZE)\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)\n",
    "\n",
    "tests_for_graph = [(x, y) for x, y in zip(x_tests[0], y_tests[0])]\n",
    "shuffle(tests_for_graph)\n",
    "\n",
    "for x, y in tests_for_graph[:10]:\n",
    "    if hyper_params[\"masking\"]:\n",
    "        encoded_x = encoder.predict(np.array(x[1]).reshape(1, int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))\n",
    "    else:\n",
    "        encoded_x = encoder.predict(np.array(x).reshape(1, hyper_params[\"input_size\"]))\n",
    "    \n",
    "    plt.hist(encoded_x.tolist()[0], bins=20)\n",
    "    plt.title(\"Encoding of a%s: %s, %s\" % (\"n Anomalous Signal\" if np.argmax(y) == 1 else \" Normal Signal\", TYPE, name))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if hyper_params[\"masking\"]:\n",
    "        plt.plot([(v - (0.5 * len(x[0])))/4.096 for v in range(len(x[0]))], x[0], label=\"Original Time Series\")\n",
    "        plt.plot([(v - (0.5 * len(x[0])))/4.096 for v in range(len(x[0]))], decoder.predict(encoded_x).tolist()[0], label=\"Reproduction Time Series\")\n",
    "    else:\n",
    "        plt.plot([(v - (0.5 * len(x)))/4.096 for v in range(len(x))], x, label=\"Original Time Series\")\n",
    "        plt.plot([(v - (0.5 * len(x)))/4.096 for v in range(len(x))], decoder.predict(encoded_x).tolist()[0], label=\"Reproduction Time Series\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(\"Reproduction of a%s: %s, %s\" % (\"n Anomalous Signal\" if np.argmax(y) == 1 else \" Normal Signal\", TYPE, name))\n",
    "    plt.xlabel(\"Time (Milliseconds)\")\n",
    "    plt.ylabel(\"Strain\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to file\n",
    "\n",
    "try:\n",
    "    previous_trials = pd.read_csv(\"ae_trials.csv\").to_dict(\"records\")\n",
    "except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "    previous_trials = []\n",
    "\n",
    "previous_trials.append(model_final_stats)\n",
    "pd.DataFrame(previous_trials).to_csv(\"ae_trials.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2115417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Temp Lock\n",
    "import os\n",
    "\n",
    "os.remove(\"failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
