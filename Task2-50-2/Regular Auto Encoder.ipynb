{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f98d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20231112163253\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "import string\n",
    "from random import choices, choice\n",
    "from datetime import datetime\n",
    "\n",
    "INPUT_SIZE = [512, 1024, 2048, 4096]\n",
    "HIDDEN_LAYERS = [1, 2]\n",
    "HIDDEN_LAYER_DIVISOR = [2, 4, 8]\n",
    "Z_LAYER_DIVISOR = [4, 8, 16, 32]\n",
    "DROPOUT = [.1, .25, .5]\n",
    "LEARNING_RATE = [0.000001, 0.00001, 0.0001]\n",
    "BATCH_SIZE = [8, 16, 32]\n",
    "FOLLOWER = True # Set later, false for aae, true for regular\n",
    "\n",
    "DATA_SET = \"Task 2 (50% Noise)\"\n",
    "TYPE = \"Vanilla AE\"\n",
    "TIME_LENGTH = 3\n",
    "DIFFERENCE_THRESHS = [.8 + 0.2 * i for i in range(25)]\n",
    "ATTEMPT = 0 # Set later, doesn't make a difference\n",
    "\n",
    "# New masking section\n",
    "MASKING = [True, False, True, True]\n",
    "MASK_SIZE = [.25, .5, .75]\n",
    "\n",
    "name = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "model_final_stats = {\"name\": name, \"dataset\": DATA_SET, \"type\": TYPE, \"len\": TIME_LENGTH}\n",
    "\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665d95ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crash' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfailed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'failed'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m FOLLOWER:\n\u001b[0;32m---> 11\u001b[0m         \u001b[43mcrash\u001b[49m()\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m         previous_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'crash' is not defined"
     ]
    }
   ],
   "source": [
    "# Assign whether follower or not based on the previous models attempt\n",
    "import pandas as pd\n",
    "\n",
    "# Previous failed\n",
    "previous_failed = False\n",
    "\n",
    "try:\n",
    "    open(\"failed\", \"x\")\n",
    "except FileExistsError:\n",
    "    if FOLLOWER:\n",
    "        crash()\n",
    "    else:\n",
    "        previous_failed = True\n",
    "\n",
    "try:\n",
    "    previous_trials = pd.read_csv(\"ae_trials.csv\").to_dict(\"records\")\n",
    "    if len(previous_trials) % 2 == 1:\n",
    "        print(1)\n",
    "        FOLLOWER = True\n",
    "        ATTEMPT = previous_trials[-1][\"attempt\"]\n",
    "    elif previous_trials[-1][\"attempt\"] == 4:\n",
    "        print(2)\n",
    "        FOLLOWER = False\n",
    "        ATTEMPT = 0\n",
    "    else:\n",
    "        print(3)\n",
    "        FOLLOWER = True\n",
    "        ATTEMPT = previous_trials[-1][\"attempt\"] + 1\n",
    "    \n",
    "    if previous_failed:\n",
    "        print(\"Prev Failed\")\n",
    "        FOLLOWER = False\n",
    "        ATTEMPT = 0\n",
    "        \n",
    "        \n",
    "except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "    print(4)\n",
    "    FOLLOWER = False\n",
    "    ATTEMPT = 0\n",
    "\n",
    "model_final_stats[\"attempt\"] = ATTEMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57d87bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '20231112163253', 'dataset': 'Task 1', 'type': 'Vanilla AE', 'len': 1, 'leader': 20231112154059, 'input_size': 4096, 'hidden_layers': 1, 'hidden_layers_divisor': 4, 'z_layer_divisor': 8, 'dropout': 0.1, 'learning_rate': 1e-05, 'batch_size': 32, 'masking': False, 'mask_size': 0.75}\n"
     ]
    }
   ],
   "source": [
    "# Pick hyperparameters\n",
    "hyper_params = {\n",
    "    \"input_size\": choice(INPUT_SIZE),\n",
    "    \"hidden_layers\": choice(HIDDEN_LAYERS),\n",
    "    \"hidden_layers_divisor\": choice(HIDDEN_LAYER_DIVISOR),\n",
    "    \"z_layer_divisor\": choice(Z_LAYER_DIVISOR),\n",
    "    \"dropout\": choice(DROPOUT),\n",
    "    \"learning_rate\": choice(LEARNING_RATE),\n",
    "    \"batch_size\": choice(BATCH_SIZE),\n",
    "    \"masking\": choice(MASKING),\n",
    "    \"mask_size\": choice(MASK_SIZE)\n",
    "}\n",
    "hyper_params[\"input_size\"] *= TIME_LENGTH\n",
    "hyper_params[\"hidden_layers_divisor\"] *= TIME_LENGTH\n",
    "hyper_params[\"z_layer_divisor\"] *= TIME_LENGTH\n",
    "model_final_stats[\"leader\"] = None\n",
    "\n",
    "if FOLLOWER:\n",
    "    previous_models = pd.read_csv(\"ae_trials.csv\", keep_default_na=False).to_dict(\"records\")\n",
    "    for model in previous_models[::-1]: # We want to find the original leader not a fake leader\n",
    "        if not model[\"leader\"]:\n",
    "            model_final_stats[\"leader\"] = model[\"name\"]\n",
    "            break\n",
    "    \n",
    "    for key in hyper_params.keys():\n",
    "        hyper_params[key] = model[key]\n",
    "\n",
    "model_final_stats.update(hyper_params)\n",
    "print(model_final_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "541664c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "from pickle import load\n",
    "from random import sample\n",
    "\n",
    "x_tests, y_tests = load(open(\"data/test_%d_%d.pickle\" % (model_final_stats[\"input_size\"], model_final_stats[\"len\"]), \"rb\"))\n",
    "x_tests = [[x - 0.5 for x in sample] for sample in x_tests[ATTEMPT]]\n",
    "y_tests = y_tests[ATTEMPT]\n",
    "\n",
    "x_train = load(open(\"data/train_%d_%d.pickle\" % (model_final_stats[\"input_size\"], model_final_stats[\"len\"]), \"rb\"))\n",
    "x_train = [[x - 0.5 for x in sample] for sample in x_train[ATTEMPT]]\n",
    "\n",
    "# Mask x_tests and y_tests\n",
    "if hyper_params[\"masking\"]:\n",
    "    # print(int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"]), hyper_params[\"input_size\"])\n",
    "    # print(len(x_tests))\n",
    "    x_tests = [[x, [x[i] for i in sorted(sample(range(len(x)), int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))]] for x in x_tests]\n",
    "    x_train = [[x, [x[i] for i in sorted(sample(range(len(x)), int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))]] for x in x_train]\n",
    "\n",
    "x_tests, y_tests = [x_tests], [y_tests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "336ab493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 16:33:03.075839: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-12 16:33:03.148755: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-12 16:33:03.870918: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               2097664   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18878976 (72.02 MB)\n",
      "Trainable params: 18878976 (72.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 16:33:04.757946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 901 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:18:00.0, compute capability: 8.9\n",
      "2023-11-12 16:33:04.758606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 21810 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:af:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Create encoder model\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "encoder = models.Sequential()\n",
    "\n",
    "if hyper_params[\"masking\"]:\n",
    "    encoder.add(layers.Input(shape=int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))\n",
    "else:\n",
    "    encoder.add(layers.Input(shape=hyper_params[\"input_size\"]))\n",
    "\n",
    "for i in range(hyper_params[\"hidden_layers\"]):\n",
    "    encoder.add(layers.Dense(hyper_params[\"input_size\"]//(hyper_params[\"hidden_layers_divisor\"] * i + 1), activation=\"elu\"))\n",
    "    encoder.add(layers.Dropout(hyper_params[\"dropout\"]))\n",
    "encoder.add(layers.Dense(hyper_params[\"input_size\"]//hyper_params[\"z_layer_divisor\"]))\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9434c80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 4096)              2101248   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18882560 (72.03 MB)\n",
      "Trainable params: 18882560 (72.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create decoder model\n",
    "\n",
    "decoder = models.Sequential()\n",
    "decoder.add(layers.Input(hyper_params[\"input_size\"]//hyper_params[\"z_layer_divisor\"]))\n",
    "for i in list(range(hyper_params[\"hidden_layers\"]))[::-1]:\n",
    "    decoder.add(layers.Dense(hyper_params[\"input_size\"]//(hyper_params[\"hidden_layers_divisor\"] * i + 1), activation=\"elu\"))\n",
    "    decoder.add(layers.Dropout(hyper_params[\"dropout\"]))\n",
    "decoder.add(layers.Dense(hyper_params[\"input_size\"]))\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e5cbafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 512)               18878976  \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 4096)              18882560  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37761536 (144.05 MB)\n",
      "Trainable params: 37761536 (144.05 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create autoencoder model\n",
    "\n",
    "autoencoder = models.Sequential()\n",
    "\n",
    "if hyper_params[\"masking\"]:\n",
    "    autoencoder.add(layers.Input(shape=int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))\n",
    "else:\n",
    "    autoencoder.add(layers.Input(shape=hyper_params[\"input_size\"]))\n",
    "\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f44aa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 16:37:30.964812: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb0b4046100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-12 16:37:30.964861: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2023-11-12 16:37:30.964875: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-11-12 16:37:30.986385: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-12 16:37:31.028510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8904\n",
      "2023-11-12 16:37:31.142066: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:231] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.9\n",
      "2023-11-12 16:37:31.142088: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Used ptxas at ptxas\n",
      "2023-11-12 16:37:31.167289: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 515ms/step - loss: 0.0243 - val_loss: 0.0225\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.0219 - val_loss: 0.0206\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.0200 - val_loss: 0.0192\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.0187 - val_loss: 0.0181\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.0178 - val_loss: 0.0174\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.0171 - val_loss: 0.0167\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.0166 - val_loss: 0.0161\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.0160 - val_loss: 0.0155\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 0.0154 - val_loss: 0.0148\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.0148 - val_loss: 0.0141\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.0142 - val_loss: 0.0134\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.0135 - val_loss: 0.0126\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 0.0128 - val_loss: 0.0118\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.0121 - val_loss: 0.0110\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.0114 - val_loss: 0.0102\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.0107 - val_loss: 0.0095\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.0100 - val_loss: 0.0088\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.0094 - val_loss: 0.0081\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.0089 - val_loss: 0.0075\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 0.0083 - val_loss: 0.0070\n",
      "Epoch 21/300\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0088"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     x_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(x_train)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(x_train), hyper_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 24\u001b[0m     fit_run \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mae_saved/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n\u001b[1;32m     34\u001b[0m final_loss \u001b[38;5;241m=\u001b[39m fit_run\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py:1809\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1805\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1806\u001b[0m     }\n\u001b[1;32m   1807\u001b[0m     epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n\u001b[0;32m-> 1809\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_logs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1810\u001b[0m training_logs \u001b[38;5;241m=\u001b[39m epoch_logs\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/callbacks.py:453\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    451\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_logs(logs)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 453\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/callbacks.py:2102\u001b[0m, in \u001b[0;36mEarlyStopping.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   2100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_epoch \u001b[38;5;241m=\u001b[39m epoch\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_best_weights:\n\u001b[0;32m-> 2102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;66;03m# Only restart wait if we beat both the baseline and our previous\u001b[39;00m\n\u001b[1;32m   2104\u001b[0m \u001b[38;5;66;03m# best.\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbaseline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_improvement(\n\u001b[1;32m   2106\u001b[0m     current, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbaseline\n\u001b[1;32m   2107\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py:2944\u001b[0m, in \u001b[0;36mModel.get_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2938\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the weights of the model.\u001b[39;00m\n\u001b[1;32m   2939\u001b[0m \n\u001b[1;32m   2940\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m   2941\u001b[0m \u001b[38;5;124;03m    A flat list of Numpy arrays.\u001b[39;00m\n\u001b[1;32m   2942\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 2944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py:1888\u001b[0m, in \u001b[0;36mLayer.get_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1886\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1887\u001b[0m         output_weights\u001b[38;5;241m.\u001b[39mappend(weight)\n\u001b[0;32m-> 1888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/backend.py:4250\u001b[0m, in \u001b[0;36mbatch_get_value\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   4238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the value of more than one tensor variable.\u001b[39;00m\n\u001b[1;32m   4239\u001b[0m \n\u001b[1;32m   4240\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4247\u001b[0m \u001b[38;5;124;03m    RuntimeError: If this method is called inside defun.\u001b[39;00m\n\u001b[1;32m   4248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m-> 4250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [x\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[1;32m   4251\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m   4252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get value inside Tensorflow graph function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/backend.py:4250\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   4238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the value of more than one tensor variable.\u001b[39;00m\n\u001b[1;32m   4239\u001b[0m \n\u001b[1;32m   4240\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4247\u001b[0m \u001b[38;5;124;03m    RuntimeError: If this method is called inside defun.\u001b[39;00m\n\u001b[1;32m   4248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m-> 4250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[1;32m   4251\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m   4252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get value inside Tensorflow graph function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:688\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    687\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    690\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1141\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m \n\u001b[1;32m   1120\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1141\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1107\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1106\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit model and save weights\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import optimizers\n",
    "import numpy as np\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=hyper_params[\"learning_rate\"])\n",
    "autoencoder.compile(opt, loss=\"mse\")\n",
    "\n",
    "if hyper_params[\"masking\"]:\n",
    "    y_train = np.array([x[0] for x in x_train]).reshape(len(x_train), hyper_params[\"input_size\"])\n",
    "    x_train = np.array([x[1] for x in x_train]).reshape(len(x_train), int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"]))\n",
    "    \n",
    "    fit_run = autoencoder.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=300,\n",
    "        batch_size=hyper_params[\"batch_size\"],\n",
    "        callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)], validation_split=0.1\n",
    "    )\n",
    "else:\n",
    "    x_train = np.array(x_train).reshape(len(x_train), hyper_params[\"input_size\"])\n",
    "    \n",
    "    fit_run = autoencoder.fit(\n",
    "        x_train,\n",
    "        x_train,\n",
    "        epochs=300,\n",
    "        batch_size=hyper_params[\"batch_size\"],\n",
    "        callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)], validation_split=0.1\n",
    "    )\n",
    "\n",
    "\n",
    "autoencoder.save_weights(\"ae_saved/%s\" % name)\n",
    "final_loss = fit_run.history[\"val_loss\"][-1]\n",
    "\n",
    "ae_res = {\"gen_loss\": None, \"disc_loss\": None, \"val_loss\": fit_run.history[\"val_loss\"][-1], \"epochs\": len(fit_run.history[\"val_loss\"])}\n",
    "model_final_stats.update(ae_res)\n",
    "\n",
    "print(ae_res)\n",
    "plt.plot(range(len(fit_run.history[\"val_loss\"])), fit_run.history[\"val_loss\"])\n",
    "plt.title(\"Autoencoder Loss: %s, %s\" % (TYPE, name))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Loss (MSE)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf643d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "(0, 1)\n",
      "Saved z-layers for 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 16:33:09.329657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'final_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m y_pred, y_true, y_class \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs, x_test, class_true):\n\u001b[1;32m     41\u001b[0m             results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(y_class)\n\u001b[0;32m---> 42\u001b[0m             results[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mse(y_pred, y_true)\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m>\u001b[39m sigma \u001b[38;5;241m*\u001b[39m \u001b[43mfinal_loss\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#             mses.append(mse(y_pred, y_true).numpy())\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#         print(mean(mses))\u001b[39;00m\n\u001b[1;32m     45\u001b[0m         save_results(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReconstructionThreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigma\u001b[39m\u001b[38;5;124m\"\u001b[39m: sigma}, \u001b[38;5;28mlist\u001b[39m(precision_recall_fscore_support(results[\u001b[38;5;241m0\u001b[39m], results[\u001b[38;5;241m1\u001b[39m], average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m))[:\u001b[38;5;241m3\u001b[39m], name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_loss' is not defined"
     ]
    }
   ],
   "source": [
    "# Save splits of z layers, run difference classifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from statistics import mean\n",
    "\n",
    "mse = MeanSquaredError()\n",
    "\n",
    "# Copied from latent layer classifiers\n",
    "def save_results(name, hyper_params, metrics, model_name):\n",
    "    try:\n",
    "        previous_trials = pd.read_csv(\"latent_trials.csv\").to_dict(\"records\")\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        previous_trials = []\n",
    "\n",
    "    model_final_stats = {\"Classifier\": name, \"Based on AE\": model_name}\n",
    "    model_final_stats.update(hyper_params)\n",
    "    \n",
    "    model_final_stats[\"precision\"] = metrics[0]\n",
    "    model_final_stats[\"recall\"] = metrics[1]\n",
    "    model_final_stats[\"f-score\"] = metrics[2]\n",
    "    \n",
    "    previous_trials.append(model_final_stats)\n",
    "    pd.DataFrame(previous_trials).to_csv(\"latent_trials.csv\", index=None)\n",
    "    \n",
    "    print(model_final_stats)\n",
    "\n",
    "for split, x_test, y_test in zip(range(len(x_tests)), x_tests, y_tests):\n",
    "\n",
    "    if hyper_params[\"masking\"]:\n",
    "        z_layers = encoder.predict(np.array([x[1] for x in x_test]).reshape(len(x_test), int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))\n",
    "    else:\n",
    "        z_layers = encoder.predict(np.array(x_test).reshape(len(x_test), hyper_params[\"input_size\"]))\n",
    "    \n",
    "    outputs = decoder.predict(z_layers).tolist()\n",
    "    print(y_test[0])\n",
    "    class_true = [[np.argmax(y)] for y in y_test]\n",
    "    \n",
    "    z_layers = [y + z.tolist() for z, y in zip(z_layers, class_true)]\n",
    "    pd.DataFrame(z_layers).to_pickle(\"z_layers/%s.pickle.gzip\" % (name))\n",
    "    print(\"Saved z-layers for %s\" % split)\n",
    "    \n",
    "    for i, sigma in enumerate(DIFFERENCE_THRESHS):\n",
    "        results = [[], []]\n",
    "        for y_pred, y_true, y_class in zip(outputs, x_test, class_true):\n",
    "            results[0].append(y_class)\n",
    "            results[1].append(1 if mse(y_pred, y_true[0]).numpy() > sigma * final_loss else 0)\n",
    "        save_results(\"ReconstructionThreshold\", {\"sigma\": sigma}, list(precision_recall_fscore_support(results[0], results[1], average=\"binary\"))[:3], name)\n",
    "    print(\"Done with split %s\" % split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97eedf61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_tests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot five graphs\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shuffle\n\u001b[0;32m----> 4\u001b[0m tests_for_graph \u001b[38;5;241m=\u001b[39m [(x, y) \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mx_tests\u001b[49m[\u001b[38;5;241m0\u001b[39m], y_tests[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m      5\u001b[0m shuffle(tests_for_graph)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m tests_for_graph[:\u001b[38;5;241m10\u001b[39m]:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_tests' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot five graphs\n",
    "from random import shuffle\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc(\"font\", size=MEDIUM_SIZE)\n",
    "plt.rc(\"axes\", titlesize=MEDIUM_SIZE, labelsize=MEDIUM_SIZE)\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)\n",
    "\n",
    "tests_for_graph = [(x, y) for x, y in zip(x_tests[0], y_tests[0])]\n",
    "shuffle(tests_for_graph)\n",
    "\n",
    "for x, y in tests_for_graph[:10]:\n",
    "    if hyper_params[\"masking\"]:\n",
    "        encoded_x = encoder.predict(np.array(x[1]).reshape(1, int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))\n",
    "    else:\n",
    "        encoded_x = encoder.predict(np.array(x).reshape(1, hyper_params[\"input_size\"]))\n",
    "    \n",
    "    plt.hist(encoded_x.tolist()[0], bins=20)\n",
    "    plt.title(\"Encoding of a%s: %s, %s\" % (\"n Anomalous Signal\" if np.argmax(y) == 1 else \" Normal Signal\", TYPE, name))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if hyper_params[\"masking\"]:\n",
    "        plt.plot([(v - (0.5 * len(x[0])))/4.096 for v in range(len(x[0]))], x[0], label=\"Original Time Series\")\n",
    "        plt.plot([(v - (0.5 * len(x[0])))/4.096 for v in range(len(x[0]))], decoder.predict(encoded_x).tolist()[0], label=\"Reproduction Time Series\")\n",
    "    else:\n",
    "        plt.plot([(v - (0.5 * len(x)))/4.096 for v in range(len(x))], x, label=\"Original Time Series\")\n",
    "        plt.plot([(v - (0.5 * len(x)))/4.096 for v in range(len(x))], decoder.predict(encoded_x).tolist()[0], label=\"Reproduction Time Series\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(\"Reproduction of a%s: %s, %s\" % (\"n Anomalous Signal\" if np.argmax(y) == 1 else \" Normal Signal\", TYPE, name))\n",
    "    plt.xlabel(\"Time (Milliseconds)\")\n",
    "    plt.ylabel(\"Strain\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to file\n",
    "\n",
    "try:\n",
    "    previous_trials = pd.read_csv(\"ae_trials.csv\").to_dict(\"records\")\n",
    "except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "    previous_trials = []\n",
    "\n",
    "previous_trials.append(model_final_stats)\n",
    "pd.DataFrame(previous_trials).to_csv(\"ae_trials.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7665680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Temp Lock\n",
    "import os\n",
    "\n",
    "os.remove(\"failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
