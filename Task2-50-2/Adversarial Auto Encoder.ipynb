{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f98d6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (466040818.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 23\u001b[0;36m\u001b[0m\n\u001b[0;31m    MASKING = [False, True, True True]\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "import string\n",
    "from random import choices, choice\n",
    "from datetime import datetime\n",
    "\n",
    "INPUT_SIZE = [512, 1024, 2048, 4096]\n",
    "INPUT_SIZE = [4096]\n",
    "HIDDEN_LAYERS = [1, 2]\n",
    "HIDDEN_LAYER_DIVISOR = [2, 4, 8]\n",
    "Z_LAYER_DIVISOR = [4, 8, 16, 32]\n",
    "DROPOUT = [.1, .25, .5]\n",
    "LEARNING_RATE = [0.000001, 0.00001, 0.0001]\n",
    "BATCH_SIZE = [8, 16, 32]\n",
    "FOLLOWER = False # Set later, false for aae, true for regular\n",
    "\n",
    "DATA_SET = \"Task 2 (50% Noise)\"\n",
    "TYPE = \"Adversarial AE\"\n",
    "TIME_LENGTH = 3\n",
    "DIFFERENCE_THRESHS = [.8 + 0.2 * i for i in range(25)]\n",
    "ATTEMPT = 0 # Set later, doesn't make a difference\n",
    "\n",
    "# New masking section\n",
    "MASKING = [False, True, True, True]\n",
    "MASK_SIZE = [.25, .5, .75]\n",
    "\n",
    "name = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "model_final_stats = {\"name\": name, \"dataset\": DATA_SET, \"type\": TYPE, \"len\": TIME_LENGTH}\n",
    "\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e13afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign whether follower or not based on the previous models attempt\n",
    "import pandas as pd\n",
    "\n",
    "# Previous failed\n",
    "previous_failed = False\n",
    "\n",
    "try:\n",
    "    open(\"failed\", \"x\")\n",
    "except FileExistsError:\n",
    "    if FOLLOWER:\n",
    "        crash()\n",
    "    else:\n",
    "        previous_failed = True\n",
    "\n",
    "try:\n",
    "    previous_trials = pd.read_csv(\"ae_trials.csv\").to_dict(\"records\")\n",
    "    if len(previous_trials) % 2 == 1:\n",
    "        print(1)\n",
    "        FOLLOWER = True\n",
    "        ATTEMPT = previous_trials[-1][\"attempt\"]\n",
    "    elif previous_trials[-1][\"attempt\"] == 4:\n",
    "        print(2)\n",
    "        FOLLOWER = False\n",
    "        ATTEMPT = 0\n",
    "    else:\n",
    "        print(3)\n",
    "        FOLLOWER = True\n",
    "        ATTEMPT = previous_trials[-1][\"attempt\"] + 1\n",
    "    \n",
    "    if previous_failed:\n",
    "        print(\"Prev Failed\")\n",
    "        FOLLOWER = False\n",
    "        ATTEMPT = 0\n",
    "        \n",
    "        \n",
    "except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "    print(4)\n",
    "    FOLLOWER = False\n",
    "    ATTEMPT = 0\n",
    "\n",
    "model_final_stats[\"attempt\"] = ATTEMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0845994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick hyperparameters\n",
    "hyper_params = {\n",
    "    \"input_size\": choice(INPUT_SIZE),\n",
    "    \"hidden_layers\": choice(HIDDEN_LAYERS),\n",
    "    \"hidden_layers_divisor\": choice(HIDDEN_LAYER_DIVISOR),\n",
    "    \"z_layer_divisor\": choice(Z_LAYER_DIVISOR),\n",
    "    \"dropout\": choice(DROPOUT),\n",
    "    \"learning_rate\": choice(LEARNING_RATE),\n",
    "    \"batch_size\": choice(BATCH_SIZE),\n",
    "    \"masking\": choice(MASKING),\n",
    "    \"mask_size\": choice(MASK_SIZE)\n",
    "}\n",
    "hyper_params[\"input_size\"] *= TIME_LENGTH\n",
    "hyper_params[\"hidden_layers_divisor\"] *= TIME_LENGTH\n",
    "hyper_params[\"z_layer_divisor\"] *= TIME_LENGTH\n",
    "model_final_stats[\"leader\"] = None\n",
    "\n",
    "if FOLLOWER:\n",
    "    previous_models = pd.read_csv(\"ae_trials.csv\", keep_default_na=False).to_dict(\"records\")\n",
    "    for model in previous_models[::-1]: # We want to find the original leader not a fake leader\n",
    "        if not model[\"leader\"]:\n",
    "            model_final_stats[\"leader\"] = model[\"name\"]\n",
    "            break\n",
    "    \n",
    "    for key in hyper_params.keys():\n",
    "        hyper_params[key] = model[key]\n",
    "\n",
    "model_final_stats.update(hyper_params)\n",
    "print(model_final_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f0c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "from pickle import load\n",
    "from random import sample\n",
    "\n",
    "x_tests, y_tests = load(open(\"data/test_%d_%d.pickle\" % (model_final_stats[\"input_size\"], model_final_stats[\"len\"]), \"rb\"))\n",
    "x_tests = [[x - 0.5 for x in sample] for sample in x_tests[ATTEMPT]]\n",
    "y_tests = y_tests[ATTEMPT]\n",
    "\n",
    "x_train = load(open(\"data/train_%d_%d.pickle\" % (model_final_stats[\"input_size\"], model_final_stats[\"len\"]), \"rb\"))\n",
    "x_train = [[x - 0.5 for x in sample] for sample in x_train[ATTEMPT]]\n",
    "\n",
    "# Mask x_tests and y_tests\n",
    "if hyper_params[\"masking\"]:\n",
    "    # print(int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"]), hyper_params[\"input_size\"])\n",
    "    # print(len(x_tests))\n",
    "    x_tests = [[x, [x[i] for i in sorted(sample(range(len(x)), int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))]] for x in x_tests]\n",
    "    x_train = [[x, [x[i] for i in sorted(sample(range(len(x)), int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))]] for x in x_train]\n",
    "\n",
    "x_tests, y_tests = [x_tests], [y_tests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ab493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder model\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "encoder = models.Sequential()\n",
    "\n",
    "if hyper_params[\"masking\"]:\n",
    "    encoder.add(layers.Input(shape=int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))\n",
    "else:\n",
    "    encoder.add(layers.Input(shape=hyper_params[\"input_size\"]))\n",
    "\n",
    "for i in range(hyper_params[\"hidden_layers\"]):\n",
    "    encoder.add(layers.Dense(hyper_params[\"input_size\"]//(hyper_params[\"hidden_layers_divisor\"] * i + 1), activation=\"elu\"))\n",
    "    encoder.add(layers.Dropout(hyper_params[\"dropout\"]))\n",
    "encoder.add(layers.Dense(hyper_params[\"input_size\"]//hyper_params[\"z_layer_divisor\"]))\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9434c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decoder model\n",
    "decoder = models.Sequential()\n",
    "decoder.add(layers.Input(hyper_params[\"input_size\"]//hyper_params[\"z_layer_divisor\"]))\n",
    "for i in list(range(hyper_params[\"hidden_layers\"]))[::-1]:\n",
    "    decoder.add(layers.Dense(hyper_params[\"input_size\"]//(hyper_params[\"hidden_layers_divisor\"] * i + 1), activation=\"elu\"))\n",
    "    decoder.add(layers.Dropout(hyper_params[\"dropout\"]))\n",
    "decoder.add(layers.Dense(hyper_params[\"input_size\"]))\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5cbafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create autoencoder model\n",
    "autoencoder = models.Sequential()\n",
    "\n",
    "if hyper_params[\"masking\"]:\n",
    "    autoencoder.add(layers.Input(shape=int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))\n",
    "else:\n",
    "    autoencoder.add(layers.Input(shape=hyper_params[\"input_size\"]))\n",
    "\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813dd5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create discriminator\n",
    "discriminator = models.Sequential()\n",
    "discriminator.add(layers.Input(shape=hyper_params[\"input_size\"]//hyper_params[\"z_layer_divisor\"]))\n",
    "for i in list(range(hyper_params[\"hidden_layers\"]))[::-1]:\n",
    "    discriminator.add(layers.Dense(hyper_params[\"input_size\"]//(hyper_params[\"hidden_layers_divisor\"] * i + 1), activation=\"elu\"))\n",
    "    discriminator.add(layers.Dropout(hyper_params[\"dropout\"]))\n",
    "discriminator.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e73e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 10:48:27.387002: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-29 10:48:27.434757: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-29 10:48:28.164525: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hyper_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m ae_opt \u001b[38;5;241m=\u001b[39m optimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[43mhyper_params\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      8\u001b[0m gen_opt \u001b[38;5;241m=\u001b[39m optimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mhyper_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      9\u001b[0m disc_opt \u001b[38;5;241m=\u001b[39m optimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mhyper_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hyper_params' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\n",
    "from tensorflow.keras.models import clone_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "ae_opt = optimizers.Adam(learning_rate=hyper_params[\"learning_rate\"])\n",
    "gen_opt = optimizers.Adam(learning_rate=hyper_params[\"learning_rate\"])\n",
    "disc_opt = optimizers.Adam(learning_rate=hyper_params[\"learning_rate\"])\n",
    "\n",
    "bin_cross_entropy = BinaryCrossentropy(from_logits=False)\n",
    "mse = MeanSquaredError()\n",
    "\n",
    "def generate_real_z(percent_normal, variance, batch_length):\n",
    "    z_normal = np.random.randn(batch_length, hyper_params[\"input_size\"]//hyper_params[\"z_layer_divisor\"])\n",
    "    z_random = np.random.rand(batch_length, hyper_params[\"input_size\"]//hyper_params[\"z_layer_divisor\"]) * variance * 3\n",
    "    z = percent_normal * z_normal * variance\n",
    "    return z + (1 - percent_normal) * (z_random - ( 0.5 * variance * 3))\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    return bin_cross_entropy(np.zeros_like(fake), fake) + bin_cross_entropy(np.ones_like(real), real)\n",
    "\n",
    "def generator_loss(fake):\n",
    "    return bin_cross_entropy(np.ones_like(fake), fake)\n",
    "\n",
    "def train_batch(batch_x, batch_y=None):\n",
    "    global autoencoder # for whatever reason, train_epochs needs these and I didn't see a point in only adding it to one\n",
    "    global discriminator\n",
    "    global generator\n",
    "    \n",
    "    with tf.GradientTape() as ae_tape:\n",
    "        ae_predict = autoencoder(batch_x, training=True)\n",
    "\n",
    "        if batch_y is not None:\n",
    "            ae_loss = mse(batch_y, ae_predict)\n",
    "        else:\n",
    "            ae_loss = mse(batch_x, ae_predict)\n",
    "    \n",
    "    gradient_ae = ae_tape.gradient(ae_loss, autoencoder.trainable_variables)\n",
    "    ae_opt.apply_gradients(zip(gradient_ae, autoencoder.trainable_variables))\n",
    "    \n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        fake_dist = encoder(batch_x, training=True)\n",
    "        real_dist = generate_real_z(1, 5, len(batch_x))\n",
    "        \n",
    "        real_out = discriminator(real_dist, training=True)\n",
    "        fake_out = discriminator(fake_dist, training=True)\n",
    "\n",
    "        disc_loss = discriminator_loss(real_out, fake_out)\n",
    "\n",
    "    gradient_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    disc_opt.apply_gradients(zip(gradient_disc, discriminator.trainable_variables))\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        fake_dist = encoder(batch_x, training=True)\n",
    "        real_dist = generate_real_z(1, 5, len(batch_x))\n",
    "        \n",
    "        real_out = discriminator(real_dist, training=True)\n",
    "        fake_out = discriminator(fake_dist, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_out)\n",
    "\n",
    "    gradient_gen = gen_tape.gradient(gen_loss, encoder.trainable_variables)\n",
    "    gen_opt.apply_gradients(zip(gradient_gen, encoder.trainable_variables))\n",
    "\n",
    "def train_epochs(x_train, epochs, batch_size, validation_split=0, memory_for_earlystopping=-1):\n",
    "    # Saves every x epochs, where x is save and is greater than 0\n",
    "    \n",
    "    global autoencoder # for whatever reason, it needs these\n",
    "    global discriminator\n",
    "    global generator\n",
    "    \n",
    "    return_dict = {}\n",
    "    \n",
    "    if validation_split > 0:\n",
    "        validation_split = int(len(x_train) * validation_split)\n",
    "        x_val = x_train[:validation_split]\n",
    "        \n",
    "        if hyper_params[\"masking\"]:\n",
    "            y_val = [x[0] for x in x_val]\n",
    "            y_val = np.array(y_val).reshape(validation_split, hyper_params[\"input_size\"])\n",
    "            \n",
    "            x_val = [x[1] for x in x_val]\n",
    "            x_val = np.array(x_val).reshape(validation_split, int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"]))\n",
    "        else:\n",
    "            x_val = np.array(x_val).reshape(validation_split, hyper_params[\"input_size\"])\n",
    "        \n",
    "        x_train = x_train[validation_split:]\n",
    "        return_dict[\"ae_loss\"] = []\n",
    "        return_dict[\"disc_loss\"] = []\n",
    "        return_dict[\"gen_loss\"] = []\n",
    "    \n",
    "    number_of_batches = len(x_train)//batch_size\n",
    "    \n",
    "    if memory_for_earlystopping != -1:\n",
    "            return_dict[\"saves\"] = []\n",
    "    \n",
    "    shuffle(x_train)\n",
    "    batches = [x_train[b::number_of_batches] for b in range(number_of_batches)]\n",
    "\n",
    "    if hyper_params[\"masking\"]:\n",
    "        batches_x = [[x[1] for x in b] for b in batches]\n",
    "        batches_x = [np.array(b).reshape(len(b), int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])) for b in batches_x]\n",
    "        \n",
    "        batches_y = [[x[0] for x in b] for b in batches]\n",
    "        batches_y = [np.array(b).reshape(len(b), hyper_params[\"input_size\"]) for b in batches_y]\n",
    "    else:\n",
    "        batches = [np.array(b).reshape(len(b), hyper_params[\"input_size\"]) for b in batches]\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        print(\"Epoch %s Running...\" % (e), end=\"\")\n",
    "\n",
    "        if hyper_params[\"masking\"]:\n",
    "            for bx, by in zip(batches_x, batches_y):\n",
    "                train_batch(bx, batch_y=by)\n",
    "        else:\n",
    "            for b in batches:\n",
    "                train_batch(b)\n",
    "        \n",
    "        if validation_split > 0:\n",
    "            ae_predict = autoencoder(x_val)\n",
    "            if hyper_params[\"masking\"]:\n",
    "                ae_loss = mse(y_val, ae_predict)\n",
    "            else:\n",
    "                ae_loss = mse(x_val, ae_predict)\n",
    "            return_dict[\"ae_loss\"].append(ae_loss.numpy())\n",
    "            print(\"Done; AE Loss: %s\" % return_dict[\"ae_loss\"][-1], end=\"; \")\n",
    "            \n",
    "            fake_dist = encoder(x_val)\n",
    "            real_dist = 10 * np.random.randn(len(x_val), hyper_params[\"input_size\"]//hyper_params[\"z_layer_divisor\"])\n",
    "        \n",
    "            real_out = discriminator(real_dist)\n",
    "            fake_out = discriminator(fake_dist)\n",
    "\n",
    "            disc_loss = discriminator_loss(real_out, fake_out)\n",
    "            gen_loss = generator_loss(fake_out)\n",
    "            return_dict[\"disc_loss\"].append(disc_loss.numpy())\n",
    "            return_dict[\"gen_loss\"].append(gen_loss.numpy())\n",
    "            print(\"Disc Loss: %s; Gen Loss: %s\" % (return_dict[\"disc_loss\"][-1], return_dict[\"gen_loss\"][-1]))\n",
    "        else:\n",
    "            print(\"Done;\")\n",
    "        \n",
    "        if memory_for_earlystopping != -1:\n",
    "            if len(return_dict[\"saves\"]) == memory_for_earlystopping and return_dict[\"ae_loss\"][-1] > return_dict[\"saves\"][0][0] and min([v[0] for v in return_dict[\"saves\"]]) == return_dict[\"saves\"][0][0]:\n",
    "                best_model = return_dict[\"saves\"][0]\n",
    "                autoencoder = best_model[1]\n",
    "                discriminator = best_model[2]\n",
    "                del return_dict[\"ae_loss\"][-1 * memory_for_earlystopping:]\n",
    "                del return_dict[\"disc_loss\"][-1 * memory_for_earlystopping:]\n",
    "                del return_dict[\"gen_loss\"][-1 * memory_for_earlystopping:]\n",
    "                break;\n",
    "            else:\n",
    "                return_dict[\"saves\"].append((return_dict[\"ae_loss\"][-1], clone_model(autoencoder), clone_model(discriminator)))\n",
    "                if len(return_dict[\"saves\"]) > memory_for_earlystopping:\n",
    "                    del return_dict[\"saves\"][0]\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f44aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model and save weights\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "\n",
    "fit_run = train_epochs(\n",
    "    x_train, 300, hyper_params[\"batch_size\"],\n",
    "    validation_split=0.1, memory_for_earlystopping=5\n",
    ")\n",
    "\n",
    "autoencoder.save_weights(\"ae_saved/%s\" % name)\n",
    "discriminator.save_weights(\"ae_saved/disc_%s\" % name)\n",
    "final_loss = fit_run[\"ae_loss\"][-1]\n",
    "\n",
    "ae_res = {\"gen_loss\": fit_run[\"gen_loss\"][-1], \"disc_loss\": fit_run[\"disc_loss\"][-1], \"val_loss\": final_loss, \"epochs\": len(fit_run[\"ae_loss\"])}\n",
    "model_final_stats.update(ae_res)\n",
    "\n",
    "print(ae_res)\n",
    "plt.plot(range(len(fit_run[\"ae_loss\"])), fit_run[\"ae_loss\"])\n",
    "plt.title(\"Autoencoder Loss: %s, %s\" % (TYPE, name))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Loss (MSE)\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(fit_run[\"gen_loss\"])), fit_run[\"gen_loss\"])\n",
    "plt.title(\"Generator Loss: %s, %s\" % (TYPE, name))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Loss (Binary cross-entropy)\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(fit_run[\"disc_loss\"])), fit_run[\"disc_loss\"])\n",
    "plt.title(\"Discriminator Loss: %s, %s\" % (TYPE, name))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Loss (Binary cross-entropy)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf643d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save splits of z layers, run difference classifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from statistics import mean\n",
    "\n",
    "# Copied from latent layer classifiers\n",
    "def save_results(name, hyper_params, metrics, model_name, auc=False):\n",
    "    try:\n",
    "        previous_trials = pd.read_csv(\"latent_trials.csv\").to_dict(\"records\")\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        previous_trials = []\n",
    "\n",
    "    model_final_stats = {\"Classifier\": name, \"Based on AE\": model_name}\n",
    "    model_final_stats.update(hyper_params)\n",
    "    \n",
    "    model_final_stats[\"precision\"] = metrics[0]\n",
    "    model_final_stats[\"recall\"] = metrics[1]\n",
    "    model_final_stats[\"f-score\"] = metrics[2]\n",
    "    \n",
    "    if auc:\n",
    "        model_final_stats[\"auc\"] = metrics[3]\n",
    "    \n",
    "    previous_trials.append(model_final_stats)\n",
    "    pd.DataFrame(previous_trials).to_csv(\"latent_trials.csv\", index=None)\n",
    "    \n",
    "    print(model_final_stats)\n",
    "\n",
    "for split, x_test, y_test in zip(range(len(x_tests)), x_tests, y_tests):\n",
    "\n",
    "    if hyper_params[\"masking\"]:\n",
    "        z_layers = encoder.predict(np.array([x[1] for x in x_test]).reshape(len(x_test), int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))\n",
    "    else:\n",
    "        z_layers = encoder.predict(np.array(x_test).reshape(len(x_test), hyper_params[\"input_size\"]))\n",
    "    \n",
    "    outputs = decoder.predict(z_layers).tolist()\n",
    "    print(y_test[0])\n",
    "    class_true = [[np.argmax(y)] for y in y_test]\n",
    "    \n",
    "    z_layers = [y + z.tolist() for z, y in zip(z_layers, class_true)]\n",
    "    pd.DataFrame(z_layers).to_pickle(\"z_layers/%s.pickle.gzip\" % (name))\n",
    "    print(\"Saved z-layers for %s\" % split)\n",
    "    \n",
    "    for i, sigma in enumerate(DIFFERENCE_THRESHS):\n",
    "        results = [[], []]\n",
    "        for y_pred, y_true, y_class in zip(outputs, x_test, class_true):\n",
    "            results[0].append(y_class)\n",
    "            results[1].append(1 if mse(y_pred, y_true[0]).numpy() > sigma * final_loss else 0)\n",
    "        save_results(\"ReconstructionThreshold\", {\"sigma\": sigma}, list(precision_recall_fscore_support(results[0], results[1], average=\"binary\"))[:3], name)\n",
    "    print(\"Done with split %s\" % split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eedf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot five graphs\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc(\"font\", size=MEDIUM_SIZE)\n",
    "plt.rc(\"axes\", titlesize=MEDIUM_SIZE, labelsize=MEDIUM_SIZE)\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)\n",
    "\n",
    "tests_for_graph = [(x, y) for x, y in zip(x_tests[0], y_tests[0])]\n",
    "shuffle(tests_for_graph)\n",
    "\n",
    "for x, y in tests_for_graph[:10]:\n",
    "    if hyper_params[\"masking\"]:\n",
    "        encoded_x = encoder.predict(np.array(x[1]).reshape(1, int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))\n",
    "    else:\n",
    "        encoded_x = encoder.predict(np.array(x).reshape(1, hyper_params[\"input_size\"]))\n",
    "    \n",
    "    plt.hist(encoded_x.tolist()[0], bins=20)\n",
    "    plt.title(\"Encoding of a%s: %s, %s\" % (\"n Anomalous Signal\" if np.argmax(y) == 1 else \" Normal Signal\", TYPE, name))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if hyper_params[\"masking\"]:\n",
    "        plt.plot([(v - (0.5 * len(x[0])))/4.096 for v in range(len(x[0]))], x[0], label=\"Original Time Series\")\n",
    "        plt.plot([(v - (0.5 * len(x[0])))/4.096 for v in range(len(x[0]))], decoder.predict(encoded_x).tolist()[0], label=\"Reproduction Time Series\")\n",
    "    else:\n",
    "        plt.plot([(v - (0.5 * len(x)))/4.096 for v in range(len(x))], x, label=\"Original Time Series\")\n",
    "        plt.plot([(v - (0.5 * len(x)))/4.096 for v in range(len(x))], decoder.predict(encoded_x).tolist()[0], label=\"Reproduction Time Series\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(\"Reproduction of a%s: %s, %s\" % (\"n Anomalous Signal\" if np.argmax(y) == 1 else \" Normal Signal\", TYPE, name))\n",
    "    plt.xlabel(\"Time (Milliseconds)\")\n",
    "    plt.ylabel(\"Strain\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to file\n",
    "\n",
    "try:\n",
    "    previous_trials = pd.read_csv(\"ae_trials.csv\").to_dict(\"records\")\n",
    "except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "    previous_trials = []\n",
    "\n",
    "previous_trials.append(model_final_stats)\n",
    "pd.DataFrame(previous_trials).to_csv(\"ae_trials.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2115417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Temp Lock\n",
    "import os\n",
    "\n",
    "os.remove(\"failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
