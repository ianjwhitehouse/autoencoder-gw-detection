{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f98d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240219001901\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "import string\n",
    "from random import choices, choice\n",
    "from datetime import datetime\n",
    "\n",
    "INPUT_SIZE = [512, 1024, 2048, 4096]\n",
    "HIDDEN_LAYERS = [1, 2]\n",
    "HIDDEN_LAYER_DIVISOR = [2, 4, 8]\n",
    "Z_LAYER_DIVISOR = [4, 8, 16, 32]\n",
    "DROPOUT = [.1, .25, .5]\n",
    "LEARNING_RATE = [0.000001, 0.00001, 0.0001]\n",
    "BATCH_SIZE = [8, 16, 32]\n",
    "FOLLOWER = True # Set later, false for aae, true for regular\n",
    "\n",
    "DATA_SET = \"Task 2 (10% Noise)\"\n",
    "TYPE = \"Vanilla AE\"\n",
    "TIME_LENGTH = 3\n",
    "DIFFERENCE_THRESHS = [.8 + 0.2 * i for i in range(25)]\n",
    "ATTEMPT = 0 # Set later, doesn't make a difference\n",
    "\n",
    "# New masking section\n",
    "MASKING = [True, False]\n",
    "MASK_SIZE = [.25, .5, .75]\n",
    "\n",
    "name = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "model_final_stats = {\"name\": name, \"dataset\": DATA_SET, \"type\": TYPE, \"len\": TIME_LENGTH}\n",
    "\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "665d95ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crash' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfailed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'failed'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m FOLLOWER:\n\u001b[0;32m---> 11\u001b[0m         \u001b[43mcrash\u001b[49m()\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m         previous_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'crash' is not defined"
     ]
    }
   ],
   "source": [
    "# Assign whether follower or not based on the previous models attempt\n",
    "import pandas as pd\n",
    "\n",
    "# Previous failed\n",
    "previous_failed = False\n",
    "\n",
    "try:\n",
    "    open(\"failed\", \"x\")\n",
    "except FileExistsError:\n",
    "    if FOLLOWER:\n",
    "        crash()\n",
    "    else:\n",
    "        previous_failed = True\n",
    "\n",
    "try:\n",
    "    previous_trials = pd.read_csv(\"ae_trials.csv\").to_dict(\"records\")\n",
    "    if len(previous_trials) % 2 == 1:\n",
    "        print(1)\n",
    "        FOLLOWER = True\n",
    "        ATTEMPT = previous_trials[-1][\"attempt\"]\n",
    "    elif previous_trials[-1][\"attempt\"] == 4:\n",
    "        print(2)\n",
    "        FOLLOWER = False\n",
    "        ATTEMPT = 0\n",
    "    else:\n",
    "        print(3)\n",
    "        FOLLOWER = True\n",
    "        ATTEMPT = previous_trials[-1][\"attempt\"] + 1\n",
    "    \n",
    "    if previous_failed:\n",
    "        print(\"Prev Failed\")\n",
    "        FOLLOWER = False\n",
    "        ATTEMPT = 0\n",
    "        \n",
    "        \n",
    "except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "    print(4)\n",
    "    FOLLOWER = False\n",
    "    ATTEMPT = 0\n",
    "\n",
    "model_final_stats[\"attempt\"] = ATTEMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57d87bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '20240219001901', 'dataset': 'Task 2 (10% Noise)', 'type': 'Vanilla AE', 'len': 3, 'leader': 20240219001731, 'input_size': 12288, 'hidden_layers': 2, 'hidden_layers_divisor': 12, 'z_layer_divisor': 24, 'dropout': 0.1, 'learning_rate': 0.0001, 'batch_size': 16, 'masking': True, 'mask_size': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Pick hyperparameters\n",
    "hyper_params = {\n",
    "    \"input_size\": choice(INPUT_SIZE),\n",
    "    \"hidden_layers\": choice(HIDDEN_LAYERS),\n",
    "    \"hidden_layers_divisor\": choice(HIDDEN_LAYER_DIVISOR),\n",
    "    \"z_layer_divisor\": choice(Z_LAYER_DIVISOR),\n",
    "    \"dropout\": choice(DROPOUT),\n",
    "    \"learning_rate\": choice(LEARNING_RATE),\n",
    "    \"batch_size\": choice(BATCH_SIZE),\n",
    "    \"masking\": choice(MASKING),\n",
    "    \"mask_size\": choice(MASK_SIZE)\n",
    "}\n",
    "hyper_params[\"input_size\"] *= TIME_LENGTH\n",
    "hyper_params[\"hidden_layers_divisor\"] *= TIME_LENGTH\n",
    "hyper_params[\"z_layer_divisor\"] *= TIME_LENGTH\n",
    "model_final_stats[\"leader\"] = None\n",
    "\n",
    "if FOLLOWER:\n",
    "    previous_models = pd.read_csv(\"ae_trials.csv\", keep_default_na=False).to_dict(\"records\")\n",
    "    for model in previous_models[::-1]: # We want to find the original leader not a fake leader\n",
    "        if not model[\"leader\"]:\n",
    "            model_final_stats[\"leader\"] = model[\"name\"]\n",
    "            break\n",
    "    \n",
    "    for key in hyper_params.keys():\n",
    "        hyper_params[key] = model[key]\n",
    "\n",
    "model_final_stats.update(hyper_params)\n",
    "print(model_final_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "541664c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "from pickle import load\n",
    "from random import sample\n",
    "\n",
    "x_tests, y_tests = load(open(\"data/test_%d_%d.pickle\" % (model_final_stats[\"input_size\"], model_final_stats[\"len\"]), \"rb\"))\n",
    "x_tests = [[x - 0.5 for x in sample] for sample in x_tests[ATTEMPT]]\n",
    "y_tests = y_tests[ATTEMPT]\n",
    "\n",
    "x_train = load(open(\"data/train_%d_%d.pickle\" % (model_final_stats[\"input_size\"], model_final_stats[\"len\"]), \"rb\"))\n",
    "x_train = [[x - 0.5 for x in sample] for sample in x_train[ATTEMPT]]\n",
    "\n",
    "# Mask x_tests and y_tests\n",
    "if hyper_params[\"masking\"]:\n",
    "    # print(int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"]), hyper_params[\"input_size\"])\n",
    "    # print(len(x_tests))\n",
    "    x_tests = [[x, [x[i] for i in sorted(sample(range(len(x)), int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))]] for x in x_tests]\n",
    "    x_train = [[x, [x[i] for i in sorted(sample(range(len(x)), int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))]] for x in x_train]\n",
    "\n",
    "x_tests, y_tests = [x_tests], [y_tests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ab493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder model\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "encoder = models.Sequential()\n",
    "\n",
    "if hyper_params[\"masking\"]:\n",
    "    encoder.add(layers.Input(shape=int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))\n",
    "else:\n",
    "    encoder.add(layers.Input(shape=hyper_params[\"input_size\"]))\n",
    "\n",
    "for i in range(hyper_params[\"hidden_layers\"]):\n",
    "    encoder.add(layers.Dense(hyper_params[\"input_size\"]//(hyper_params[\"hidden_layers_divisor\"] * i + 1), activation=\"elu\"))\n",
    "    encoder.add(layers.Dropout(hyper_params[\"dropout\"]))\n",
    "encoder.add(layers.Dense(hyper_params[\"input_size\"]//hyper_params[\"z_layer_divisor\"]))\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9434c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decoder model\n",
    "\n",
    "decoder = models.Sequential()\n",
    "decoder.add(layers.Input(hyper_params[\"input_size\"]//hyper_params[\"z_layer_divisor\"]))\n",
    "for i in list(range(hyper_params[\"hidden_layers\"]))[::-1]:\n",
    "    decoder.add(layers.Dense(hyper_params[\"input_size\"]//(hyper_params[\"hidden_layers_divisor\"] * i + 1), activation=\"elu\"))\n",
    "    decoder.add(layers.Dropout(hyper_params[\"dropout\"]))\n",
    "decoder.add(layers.Dense(hyper_params[\"input_size\"]))\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5cbafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create autoencoder model\n",
    "\n",
    "autoencoder = models.Sequential()\n",
    "\n",
    "if hyper_params[\"masking\"]:\n",
    "    autoencoder.add(layers.Input(shape=int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))\n",
    "else:\n",
    "    autoencoder.add(layers.Input(shape=hyper_params[\"input_size\"]))\n",
    "\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f44aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model and save weights\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import optimizers\n",
    "import numpy as np\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=hyper_params[\"learning_rate\"])\n",
    "autoencoder.compile(opt, loss=\"mse\")\n",
    "\n",
    "if hyper_params[\"masking\"]:\n",
    "    y_train = np.array([x[0] for x in x_train]).reshape(len(x_train), hyper_params[\"input_size\"])\n",
    "    x_train = np.array([x[1] for x in x_train]).reshape(len(x_train), int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"]))\n",
    "    \n",
    "    fit_run = autoencoder.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=300,\n",
    "        batch_size=hyper_params[\"batch_size\"],\n",
    "        callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)], validation_split=0.1\n",
    "    )\n",
    "else:\n",
    "    x_train = np.array(x_train).reshape(len(x_train), hyper_params[\"input_size\"])\n",
    "    \n",
    "    fit_run = autoencoder.fit(\n",
    "        x_train,\n",
    "        x_train,\n",
    "        epochs=300,\n",
    "        batch_size=hyper_params[\"batch_size\"],\n",
    "        callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)], validation_split=0.1\n",
    "    )\n",
    "\n",
    "\n",
    "autoencoder.save_weights(\"ae_saved/%s\" % name)\n",
    "final_loss = fit_run.history[\"val_loss\"][-1]\n",
    "\n",
    "ae_res = {\"gen_loss\": None, \"disc_loss\": None, \"val_loss\": fit_run.history[\"val_loss\"][-1], \"epochs\": len(fit_run.history[\"val_loss\"])}\n",
    "model_final_stats.update(ae_res)\n",
    "\n",
    "print(ae_res)\n",
    "plt.plot(range(len(fit_run.history[\"val_loss\"])), fit_run.history[\"val_loss\"])\n",
    "plt.title(\"Autoencoder Loss: %s, %s\" % (TYPE, name))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Loss (MSE)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf643d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save splits of z layers, run difference classifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from statistics import mean\n",
    "\n",
    "mse = MeanSquaredError()\n",
    "\n",
    "# Copied from latent layer classifiers\n",
    "def save_results(name, hyper_params, metrics, model_name):\n",
    "    try:\n",
    "        previous_trials = pd.read_csv(\"latent_trials.csv\").to_dict(\"records\")\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        previous_trials = []\n",
    "\n",
    "    model_final_stats = {\"Classifier\": name, \"Based on AE\": model_name}\n",
    "    model_final_stats.update(hyper_params)\n",
    "    \n",
    "    model_final_stats[\"precision\"] = metrics[0]\n",
    "    model_final_stats[\"recall\"] = metrics[1]\n",
    "    model_final_stats[\"f-score\"] = metrics[2]\n",
    "    \n",
    "    previous_trials.append(model_final_stats)\n",
    "    pd.DataFrame(previous_trials).to_csv(\"latent_trials.csv\", index=None)\n",
    "    \n",
    "    print(model_final_stats)\n",
    "\n",
    "for split, x_test, y_test in zip(range(len(x_tests)), x_tests, y_tests):\n",
    "\n",
    "    if hyper_params[\"masking\"]:\n",
    "        z_layers = encoder.predict(np.array([x[1] for x in x_test]).reshape(len(x_test), int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))\n",
    "    else:\n",
    "        z_layers = encoder.predict(np.array(x_test).reshape(len(x_test), hyper_params[\"input_size\"]))\n",
    "    \n",
    "    outputs = decoder.predict(z_layers).tolist()\n",
    "    print(y_test[0])\n",
    "    class_true = [[np.argmax(y)] for y in y_test]\n",
    "    \n",
    "    z_layers = [y + z.tolist() for z, y in zip(z_layers, class_true)]\n",
    "    pd.DataFrame(z_layers).to_pickle(\"z_layers/%s.pickle.gzip\" % (name))\n",
    "    print(\"Saved z-layers for %s\" % split)\n",
    "    \n",
    "    for i, sigma in enumerate(DIFFERENCE_THRESHS):\n",
    "        results = [[], []]\n",
    "        for y_pred, y_true, y_class in zip(outputs, x_test, class_true):\n",
    "            results[0].append(y_class)\n",
    "            results[1].append(1 if mse(y_pred, y_true[0]).numpy() > sigma * final_loss else 0)\n",
    "        save_results(\"ReconstructionThreshold\", {\"sigma\": sigma}, list(precision_recall_fscore_support(results[0], results[1], average=\"binary\"))[:3], name)\n",
    "    print(\"Done with split %s\" % split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eedf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot five graphs\n",
    "from random import shuffle\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc(\"font\", size=MEDIUM_SIZE)\n",
    "plt.rc(\"axes\", titlesize=MEDIUM_SIZE, labelsize=MEDIUM_SIZE)\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)\n",
    "\n",
    "tests_for_graph = [(x, y) for x, y in zip(x_tests[0], y_tests[0])]\n",
    "shuffle(tests_for_graph)\n",
    "\n",
    "for x, y in tests_for_graph[:10]:\n",
    "    if hyper_params[\"masking\"]:\n",
    "        encoded_x = encoder.predict(np.array(x[1]).reshape(1, int(hyper_params[\"input_size\"] * hyper_params[\"mask_size\"])))\n",
    "    else:\n",
    "        encoded_x = encoder.predict(np.array(x).reshape(1, hyper_params[\"input_size\"]))\n",
    "    \n",
    "    plt.hist(encoded_x.tolist()[0], bins=20)\n",
    "    plt.title(\"Encoding of a%s: %s, %s\" % (\"n Anomalous Signal\" if np.argmax(y) == 1 else \" Normal Signal\", TYPE, name))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if hyper_params[\"masking\"]:\n",
    "        plt.plot([(v - (0.5 * len(x[0])))/4.096 for v in range(len(x[0]))], x[0], label=\"Original Time Series\")\n",
    "        plt.plot([(v - (0.5 * len(x[0])))/4.096 for v in range(len(x[0]))], decoder.predict(encoded_x).tolist()[0], label=\"Reproduction Time Series\")\n",
    "    else:\n",
    "        plt.plot([(v - (0.5 * len(x)))/4.096 for v in range(len(x))], x, label=\"Original Time Series\")\n",
    "        plt.plot([(v - (0.5 * len(x)))/4.096 for v in range(len(x))], decoder.predict(encoded_x).tolist()[0], label=\"Reproduction Time Series\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(\"Reproduction of a%s: %s, %s\" % (\"n Anomalous Signal\" if np.argmax(y) == 1 else \" Normal Signal\", TYPE, name))\n",
    "    plt.xlabel(\"Time (Milliseconds)\")\n",
    "    plt.ylabel(\"Strain\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to file\n",
    "\n",
    "try:\n",
    "    previous_trials = pd.read_csv(\"ae_trials.csv\").to_dict(\"records\")\n",
    "except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "    previous_trials = []\n",
    "\n",
    "previous_trials.append(model_final_stats)\n",
    "pd.DataFrame(previous_trials).to_csv(\"ae_trials.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7665680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Temp Lock\n",
    "import os\n",
    "\n",
    "os.remove(\"failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
